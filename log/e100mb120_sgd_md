###########################################
#epoch number: 100
#minibatch size: 120
#precision type: full precision
#gradients update rule: SGD_Momentum_Decay
###########################################
epoch  1 : iter   50 : loss 2.361537 : vld 0.868022
epoch  1 : iter  100 : loss 2.342769 : vld 0.792701
epoch  1 : iter  150 : loss 2.325389 : vld 0.709382
epoch  1 : iter  200 : loss 2.307043 : vld 0.622063
epoch  1 : iter  250 : loss 2.289397 : vld 0.561573
epoch  1 : iter  300 : loss 2.270838 : vld 0.517914
epoch  1 : iter  350 : loss 2.254718 : vld 0.481086
epoch  1 : iter  400 : loss 2.240646 : vld 0.443093
epoch  1 : iter  450 : loss 2.222814 : vld 0.421263
epoch  2 : iter   50 : loss 2.206601 : vld 0.369772
epoch  2 : iter  100 : loss 2.189157 : vld 0.355107
epoch  2 : iter  150 : loss 2.171195 : vld 0.337444
epoch  2 : iter  200 : loss 2.157946 : vld 0.321613
epoch  2 : iter  250 : loss 2.142651 : vld 0.310948
epoch  2 : iter  300 : loss 2.121944 : vld 0.302616
epoch  2 : iter  350 : loss 2.113242 : vld 0.295784
epoch  2 : iter  400 : loss 2.092649 : vld 0.289618
epoch  2 : iter  450 : loss 2.079378 : vld 0.283619
epoch  3 : iter   50 : loss 2.062969 : vld 0.287619
epoch  3 : iter  100 : loss 2.050838 : vld 0.281286
epoch  3 : iter  150 : loss 2.030061 : vld 0.279787
epoch  3 : iter  200 : loss 2.016791 : vld 0.276954
epoch  3 : iter  250 : loss 2.005981 : vld 0.272121
epoch  3 : iter  300 : loss 1.987090 : vld 0.271288
epoch  3 : iter  350 : loss 1.975358 : vld 0.268455
epoch  3 : iter  400 : loss 1.958940 : vld 0.265956
epoch  3 : iter  450 : loss 1.946387 : vld 0.261123
epoch  4 : iter   50 : loss 1.932426 : vld 0.272121
epoch  4 : iter  100 : loss 1.918612 : vld 0.272788
epoch  4 : iter  150 : loss 1.900350 : vld 0.271455
epoch  4 : iter  200 : loss 1.887685 : vld 0.269288
epoch  4 : iter  250 : loss 1.878785 : vld 0.267455
epoch  4 : iter  300 : loss 1.854122 : vld 0.267289
epoch  4 : iter  350 : loss 1.845434 : vld 0.265456
epoch  4 : iter  400 : loss 1.830927 : vld 0.265789
epoch  4 : iter  450 : loss 1.821938 : vld 0.263623
epoch  5 : iter   50 : loss 1.807615 : vld 0.248125
epoch  5 : iter  100 : loss 1.800813 : vld 0.246126
epoch  5 : iter  150 : loss 1.781446 : vld 0.246126
epoch  5 : iter  200 : loss 1.772484 : vld 0.245292
epoch  5 : iter  250 : loss 1.756540 : vld 0.244293
epoch  5 : iter  300 : loss 1.736742 : vld 0.244626
epoch  5 : iter  350 : loss 1.727369 : vld 0.242293
epoch  5 : iter  400 : loss 1.715806 : vld 0.240793
epoch  5 : iter  450 : loss 1.695925 : vld 0.241626
epoch  6 : iter   50 : loss 1.691324 : vld 0.252791
epoch  6 : iter  100 : loss 1.679768 : vld 0.251791
epoch  6 : iter  150 : loss 1.665394 : vld 0.251125
epoch  6 : iter  200 : loss 1.659886 : vld 0.249625
epoch  6 : iter  250 : loss 1.634634 : vld 0.248625
epoch  6 : iter  300 : loss 1.636521 : vld 0.247292
epoch  6 : iter  350 : loss 1.621852 : vld 0.247625
epoch  6 : iter  400 : loss 1.602877 : vld 0.245459
epoch  6 : iter  450 : loss 1.591842 : vld 0.245792
epoch  7 : iter   50 : loss 1.587416 : vld 0.240627
epoch  7 : iter  100 : loss 1.568796 : vld 0.239793
epoch  7 : iter  150 : loss 1.569402 : vld 0.238460
epoch  7 : iter  200 : loss 1.549305 : vld 0.237627
epoch  7 : iter  250 : loss 1.525598 : vld 0.238127
epoch  7 : iter  300 : loss 1.527665 : vld 0.235794
epoch  7 : iter  350 : loss 1.518619 : vld 0.234961
epoch  7 : iter  400 : loss 1.505367 : vld 0.234294
epoch  7 : iter  450 : loss 1.499775 : vld 0.233961
epoch  8 : iter   50 : loss 1.487639 : vld 0.221630
epoch  8 : iter  100 : loss 1.467807 : vld 0.221630
